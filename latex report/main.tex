\documentclass[12pt,a4paper]{article}

% ====== Encodage / Langue ======
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}   
% ====== Mise en page ======
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{enumitem}
\setlist[itemize]{label=\raisebox{0.2ex}{\scalebox{0.8}{$\bullet$}}}

% ====== Maths ======
\usepackage{amsmath,amssymb}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
% ====== Figures ======
\usepackage{graphicx}
\usepackage{float}

% ====== Liens ======
\usepackage[hidelinks]{hyperref}

\begin{document}

% ===================== PAGE DE GARDE =======================
\begin{titlepage}
\begin{tikzpicture}[remember picture,overlay]

% Logo Université Paris-Saclay (haut gauche)
\node[anchor=north west, xshift=1cm, yshift=-1cm] 
at (current page.north west)
{\includegraphics[width=4.2cm]{images/universite.png}};

% Logo ENS Paris-Saclay (haut droite)
\node[anchor=north east, xshift=-1cm, yshift=-1cm] 
at (current page.north east)
{\includegraphics[width=4.2cm]{images/ens.png}};

% Logo Master CHPS (bas centre)
\node[anchor=south, yshift=1.5cm] 
at (current page.south)
{\includegraphics[width=6.5cm]{images/chps.png}};

\end{tikzpicture}

\vspace*{5cm}

\begin{center}

{\Huge \textbf{HPL High Performance Linpack Benchmark}\par}

\vspace{1.5cm}

{\Large Projet MPNA \\[0.2cm]
Méthodes et Programmation Numérique Avancée\par}

\vspace{2cm}

{\Large \textbf{Réalisé par :}\par}

\vspace{0.5cm}

{\Large \textit{BENMALK Achraf}\par}
\vspace{0.3cm}
{\Large \textit{BEZINE Mohamed Karim}\par}

\vspace{2cm}

{\Large \textbf{Date :} \today\par}

\end{center}

\end{titlepage}


\tableofcontents
\listoffigures
\listoftables
\newpage

% ==========================================================
\section{Contexte}
HPL (High Performance Linpack) est un benchmark fondamental en calcul haute performance qui évalue la capacité de calcul en virgule flottante d’un système. Il est largement utilisé pour mesurer l’efficacité des CPUs et des GPUs lors de la résolution de grands systèmes linéaires denses.

HPL ne mesure pas uniquement la performance brute en gigaflops ou teraflops. Il permet également d’évaluer la gestion du parallélisme, les accès mémoire ainsi que le coût des communications. Il constitue ainsi un outil essentiel pour l’optimisation et la comparaison des systèmes HPC.

% ==========================================================
\section{Caractéristiques du Benchmark HPL}

HPL mesure la vitesse de résolution d’un système linéaire dense de la forme :

\[
Ax = b
\]

où \(A\) est une matrice dense de grande taille.

Il s’agit du benchmark utilisé pour classer les supercalculateurs dans la liste TOP500.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{images/hpl-parameters.png}
    \caption{Paramètres principaux du benchmark HPL}
\end{figure}

Les paramètres principaux sont :

\begin{itemize}
    \item \textbf{N} : taille globale de la matrice.
    \item \textbf{NB} : taille des blocs.
    \item \textbf{P × Q} : grille de processus MPI.
\end{itemize}

La matrice est distribuée selon une stratégie \textbf{block-cyclic} afin d’équilibrer la charge entre les processus et de minimiser les communications.

% ==========================================================
\section{Fichier d'entrée HPL.dat}

Le benchmark nécessite un fichier de configuration nommé \texttt{HPL.dat}.  
Ce fichier définit :

\begin{itemize}
    \item Les tailles de matrices testées
    \item Les tailles de blocs
    \item La grille de processus MPI
    \item Les paramètres algorithmiques
    \item La tolérance de vérification du résidu
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.9]{images/hpl-dat.png}
    \caption{Exemple de fichier HPL.dat}
\end{figure}

HPL explore différentes combinaisons de paramètres afin d’identifier la configuration optimale.

% ==========================================================
\section{Étapes d'exécution}

Les étapes principales sont :

\begin{enumerate}
    \item Création du fichier \texttt{HPL.dat}
    \item Lancement du conteneur Singularity
    \item Allocation des ressources GPU via Slurm
    \item Exécution du benchmark avec MPI
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step1.png}
    \caption{Création du fichier HPL.dat}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step2.png}
    \caption{Binding du dossier dans le conteneur}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step3.png}
    \caption{Configuration persistante du bind}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step4.png}
    \caption{Exécution du benchmark}
\end{figure}

Commande principale :

\begin{verbatim}
mpirun -np 1 ./hpl.sh --dat /mnt/HPL.dat
\end{verbatim}

% ==========================================================
\section{Partition A100}

\subsection{1 GPU, 1 Processus MPI}

\begin{verbatim}
salloc -t 4:00:00 -n 1 --ntasks=1 -p gpu --gres=gpu:1
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.51 & 9731 \\
40000 & 1.63 & 15890 \\
60000 & 8.36 & 17150 \\
80000 & 19.01 & 17600 \\
100000 & 36.94 & 17860 \\
\hline
\end{tabular}
\caption{Résultats sur 1 GPU A100}
\end{table}

Le pic théorique FP64 de l'A100 est calculé à partir du nombre de cœurs CUDA FP64, du facteur FMA (\textit{Fused Multiply-Add}, chaque opération FMA compte pour 2 opérations flottantes) et de la fréquence boost :

\[
\text{Peak}_{A100} = \underbrace{6912}_{\text{cœurs FP64}} \times \underbrace{2}_{\text{FMA}} \times \underbrace{1{,}41 \text{ GHz}}_{\text{freq. boost}} \approx 19{,}5 \text{ TFLOPS}
\]

Efficacité maximale atteinte (à $N = 100\,000$) :

\[
\eta_{A100} = \frac{17\,860}{19\,487} \times 100 \approx 91{,}7\%
\]

Cette efficacité élevée confirme que HPL est un benchmark \textit{compute-bound} : la majorité du temps est consacrée aux opérations DGEMM (produits de matrices denses), qui exploitent pleinement les unités de calcul du GPU.

% ==========================================================
\subsection{2 GPU, 2 Processus MPI, 1 Processus MPI/GPU}

\begin{verbatim}
salloc -t 4:00:00 --ntasks=2 -p gpu --gres=gpu:2
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.55 & 9106 \\
40000 & 1.66 & 25230 \\
60000 & 4.56 & 31430 \\
80000 & 9.98 & 33560 \\
100000 & 19.00 & 34720 \\
\hline
\end{tabular}
\caption{Résultats sur 2 GPUs A100}
\end{table}

Speedup à $N = 100\,000$ :

\[
S_{A100} = \frac{34\,720}{17\,860} \approx 1{,}94
\]

Efficacité parallèle :

\[
E_{A100} = \frac{S_{A100}}{2} \times 100 = \frac{1{,}94}{2} \times 100 \approx 97\%
\]

\textbf{Remarque :} À $N = 20\,000$, les 2 GPUs (9\,106 GFLOPS) sont \textit{plus lents} que le GPU unique (9\,731 GFLOPS). Ce phénomène est analysé en section~\ref{sec:analyse}.

% ==========================================================
\section{Partition H100}

\subsection{1 GPU, 1 Processus MPI}

\begin{verbatim}
salloc -t 4:00:00 -n 1 -p gpu_h100 --gres=gpu:1
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.31 & 16130 \\
40000 & 1.17 & 35760 \\
60000 & 3.43 & 41760 \\
80000 & 7.57 & 44130 \\
100000 & 11.43 & 45110 \\
\hline
\end{tabular}
\caption{Résultats sur 1 GPU H100}
\end{table}

Le pic théorique FP64 du H100 (variante SXM) est calculé de manière analogue :

\[
\text{Peak}_{H100} = \underbrace{16\,896}_{\text{cœurs FP64}} \times \underbrace{2}_{\text{FMA}} \times \underbrace{1{,}6 \text{ GHz}}_{\text{freq. boost}} \approx 54{,}1 \text{ TFLOPS}
\]

Efficacité maximale atteinte (à $N = 100\,000$) :

\[
\eta_{H100} = \frac{45\,110}{54\,067} \times 100 \approx 83{,}4\%
\]

L'efficacité du H100 est inférieure à celle de l'A100 (83{,}4\% vs 91{,}7\%). Le H100 possède 2{,}4$\times$ plus de cœurs CUDA que l'A100, ce qui rend plus difficile la saturation complète de toutes les unités de calcul pour une même taille de problème.

% ==========================================================
\subsection{2 GPU, 2 Processus MPI, 1 Processus MPI/GPU}

\begin{verbatim}
salloc -t 4:00:00 --ntasks=2 -p gpu_h100 --gres=gpu:2
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.40 & 11510 \\
40000 & 1.01 & 41460 \\
60000 & 1.11 & 64700 \\
80000 & 4.36 & 76730 \\
100000 & 7.95 & 81970 \\
\hline
\end{tabular}
\caption{Résultats sur 2 GPUs H100}
\end{table}

Speedup à $N = 100\,000$ :

\[
S_{H100} = \frac{81\,970}{45\,110} \approx 1{,}82
\]

Efficacité parallèle :

\[
E_{H100} = \frac{S_{H100}}{2} \times 100 = \frac{1{,}82}{2} \times 100 \approx 91\%
\]

\textbf{Remarque :} Le passage multi-GPU est moins efficace sur H100 (91\%) que sur A100 (97\%). À $N = 20\,000$, les 2 GPUs H100 (11\,510 GFLOPS) sont même 28{,}6\% \textit{plus lents} que le GPU unique (16\,130 GFLOPS). Cette dégradation est analysée en section~\ref{sec:analyse}.

% ==========================================================
\section{Analyse des résultats}
\label{sec:analyse}

\subsection{Évolution des performances avec la taille du problème}

Sur les deux architectures, les performances (GFLOPS) augmentent avec $N$ avant de converger vers un plateau. Ce comportement s'explique par le rapport entre calcul et communication :

\begin{itemize}
    \item Le volume de calcul de la factorisation LU croît en $\mathcal{O}(N^3)$.
    \item Le volume de communication croît en $\mathcal{O}(N^2)$.
\end{itemize}

Ainsi, pour des grandes valeurs de $N$, le calcul domine largement et le GPU peut maintenir un taux d'utilisation élevé de ses unités de calcul (principalement via les opérations DGEMM). Pour les petites valeurs de $N$, le \textit{overhead} de communication et de synchronisation représente une fraction significative du temps total, ce qui réduit les GFLOPS mesurés.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/hpl-a100.png}
    \caption{Performances A100 : 1 GPU vs 2 GPUs}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/hpl-h100.png}
    \caption{Performances H100 : 1 GPU vs 2 GPUs}
\end{figure}

\subsection{Passage à l'échelle multi-GPU}

Le tableau~\ref{tab:scaling} résume le passage à l'échelle pour $N = 100\,000$ :

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
GPU & 1 GPU (GFLOPS) & 2 GPUs (GFLOPS) & Speedup & Efficacité parallèle \\
\hline
A100 & 17\,860 & 34\,720 & 1{,}94$\times$ & 97\% \\
H100 & 45\,110 & 81\,970 & 1{,}82$\times$ & 91\% \\
\hline
\end{tabular}
\caption{Passage à l'échelle multi-GPU à $N = 100\,000$}
\label{tab:scaling}
\end{table}

L'A100 affiche une meilleure efficacité parallèle (97\%) que le H100 (91\%). Cette différence s'explique par le fait que le H100, étant plus rapide en calcul pur, termine la phase de calcul plus rapidement, ce qui rend la latence de communication inter-GPU (via NVLink) proportionnellement plus coûteuse. Le rapport calcul/communication est donc moins favorable sur H100 pour une même taille de problème.

\subsection{Anomalie à $N = 20\,000$ : 2 GPUs plus lents qu'un seul}

Un résultat remarquable concerne la dégradation de performance à $N = 20\,000$ en configuration multi-GPU :

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
GPU & 1 GPU (GFLOPS) & 2 GPUs (GFLOPS) & Dégradation \\
\hline
A100 & 9\,731 & 9\,106 & $-6{,}4\%$ \\
H100 & 16\,130 & 11\,510 & $-28{,}6\%$ \\
\hline
\end{tabular}
\caption{Dégradation de performance à $N = 20\,000$ en multi-GPU}
\end{table}

Avec un problème de petite taille ($N = 20\,000$), chaque GPU reçoit une portion de matrice trop réduite pour saturer ses milliers de cœurs CUDA. Le coût de synchronisation et d'échange de données entre les deux GPUs dépasse alors le bénéfice du parallélisme supplémentaire. Ce phénomène est plus prononcé sur H100 ($-28{,}6\%$) car ce GPU possède davantage de cœurs à alimenter en données (16\,896 vs 6\,912).

Il existe donc un seuil de taille de problème en dessous duquel l'ajout de GPUs est contre-productif. Pour nos configurations, ce seuil se situe autour de $N = 30\,000$.

\subsection{Comparaison architecturale A100 vs H100}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/a100-h100-hpl.png}
    \caption{Comparaison des performances A100 vs H100 (1 GPU)}
\end{figure}

Le tableau~\ref{tab:speedup_arch} présente le facteur d'accélération du H100 par rapport à l'A100 pour chaque taille de problème :

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
N & A100 (GFLOPS) & H100 (GFLOPS) & Ratio H100/A100 \\
\hline
20\,000  & 9\,731  & 16\,130 & 1{,}66$\times$ \\
40\,000  & 15\,890 & 35\,760 & 2{,}25$\times$ \\
60\,000  & 17\,150 & 41\,760 & 2{,}43$\times$ \\
80\,000  & 17\,600 & 44\,130 & 2{,}51$\times$ \\
100\,000 & 17\,860 & 45\,110 & 2{,}53$\times$ \\
\hline
\end{tabular}
\caption{Facteur d'accélération H100 par rapport à A100}
\label{tab:speedup_arch}
\end{table}

Le ratio converge vers $\approx 2{,}5\times$ pour les grandes tailles de problème. Cela est cohérent avec le rapport des pics théoriques FP64 :

\[
\frac{\text{Peak}_{H100}}{\text{Peak}_{A100}} = \frac{54\,067}{19\,487} \approx 2{,}77
\]

Le facteur mesuré (2{,}53$\times$) est légèrement inférieur au facteur théorique (2{,}77$\times$). Cet écart de $\approx 9\%$ peut s'expliquer par des différences de bande passante mémoire, d'efficacité du \textit{scheduler} GPU, et du fait que le H100 est plus difficile à saturer (efficacité de 83{,}4\% vs 91{,}7\% pour l'A100).

À $N = 20\,000$, le ratio n'est que de 1{,}66$\times$ car les deux GPUs sont sous-utilisés : ni l'un ni l'autre ne peut exploiter pleinement ses unités de calcul avec un problème aussi petit.

\subsection{Synthèse de l'efficacité}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Configuration & Pic théorique (TFLOPS) & Meilleur résultat (GFLOPS) & Efficacité \\
\hline
A100 (1 GPU) & 19{,}5 & 17\,860 & 91{,}7\% \\
H100 (1 GPU) & 54{,}1 & 45\,110 & 83{,}4\% \\
A100 (2 GPUs) & 39{,}0 & 34\,720 & 89{,}0\% \\
H100 (2 GPUs) & 108{,}1 & 81\,970 & 75{,}8\% \\
\hline
\end{tabular}
\caption{Synthèse de l'efficacité HPL sur les différentes configurations}
\label{tab:efficacite}
\end{table}

On observe que l'efficacité décroît lorsqu'on passe à des configurations plus puissantes. Plus le pic théorique est élevé, plus il est difficile de l'exploiter pleinement. Néanmoins, toutes les configurations atteignent des efficacités supérieures à 75\%, ce qui témoigne de la nature \textit{compute-bound} du benchmark HPL et de la qualité de l'implémentation GPU fournie par NVIDIA dans le conteneur HPC-Benchmarks.

% ==========================================================
\section{Conclusion}

Ce travail a permis d'évaluer les performances du benchmark HPL sur deux architectures GPU NVIDIA --- l'A100 (Ampere) et le H100 (Hopper) --- au sein du supercalculateur Toubkal.

Les principaux enseignements sont :

\begin{enumerate}
    \item \textbf{HPL est un benchmark compute-bound :} Les efficacités mesurées (83--92\%) confirment que la performance est principalement limitée par la puissance de calcul et non par la bande passante mémoire ou les communications.

    \item \textbf{La taille du problème est déterminante :} Les performances augmentent significativement avec $N$ en raison du rapport $\mathcal{O}(N^3)/\mathcal{O}(N^2)$ entre calcul et communication. Un $N$ trop petit conduit à une sous-utilisation des ressources.

    \item \textbf{Le multi-GPU a un seuil de rentabilité :} En dessous d'un certain $N$ ($\approx 30\,000$ dans nos tests), l'ajout d'un second GPU dégrade les performances au lieu de les améliorer.

    \item \textbf{Le H100 offre un gain de $\approx 2{,}5\times$ sur l'A100 :} Ce ratio est cohérent avec le rapport des pics théoriques FP64 ($2{,}77\times$), avec un écart dû à la difficulté accrue de saturer les unités de calcul du H100.

    \item \textbf{L'efficacité parallèle diminue avec la puissance :} L'A100 affiche une meilleure efficacité parallèle multi-GPU (97\%) que le H100 (91\%), car la latence de communication devient proportionnellement plus coûteuse sur un GPU plus rapide.
\end{enumerate}

Ces résultats illustrent les compromis fondamentaux du calcul haute performance : puissance brute, passage à l'échelle, et dimensionnement du problème sont étroitement liés. L'optimisation des paramètres HPL (taille de matrice, taille de bloc, grille de processus) reste essentielle pour exploiter au mieux les architectures modernes.

% ==========================================================
\section*{Références}
\addcontentsline{toc}{section}{Références}

\begin{enumerate}[label={[\arabic*]}]
    \item A. Petitet, R. C. Whaley, J. Dongarra, A. Cleary, \textit{HPL --- A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers}, version 2.3, 2018. \url{https://www.netlib.org/benchmark/hpl/}

    \item TOP500 Project, \textit{TOP500 Supercomputer Sites}. \url{https://www.top500.org/}

    \item NVIDIA Corporation, \textit{NVIDIA A100 Tensor Core GPU --- Architecture Whitepaper}, 2020.

    \item NVIDIA Corporation, \textit{NVIDIA H100 Tensor Core GPU --- Architecture Whitepaper}, 2022.

    \item NVIDIA Corporation, \textit{NVIDIA HPC-Benchmarks Container}, NGC Catalog. \url{https://catalog.ngc.nvidia.com/orgs/nvidia/containers/hpc-benchmarks}
\end{enumerate}

\end{document}
