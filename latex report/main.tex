% ============================================================
%  Rapport principal — HPL High Performance Linpack Benchmark
%  + Section intégrée : Mini-HPL parallèle avec MPI (pédagogique)
%  Overleaf ready — Compiler: pdflatex (2 passes)
% ============================================================

\documentclass[12pt,a4paper]{article}

% ====== Encodage / Langue ======
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

% ====== Mise en page ======
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}

\usepackage{enumitem}
\setlist[itemize]{label=\raisebox{0.2ex}{\scalebox{0.8}{$\bullet$}}}

% ====== Maths ======
\usepackage{amsmath,amssymb}
\usepackage{bm}

% ====== Figures / Graphiques ======
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}

% ====== Table des matières stylée (points) ======
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

% ====== Tableaux ======
\usepackage{booktabs}
\usepackage{array}

% ====== Code (Mini-HPL) ======
\usepackage{listings}
\usepackage{xcolor}

% ====== Boîtes (Mini-HPL) ======
\usepackage{tcolorbox}
\tcbuselibrary{breakable, skins}

% ====== En-têtes ======
\usepackage{fancyhdr}

% ====== Liens ======
\usepackage[hidelinks]{hyperref}
% (si tu veux des liens en bleu: remplace hidelinks par)
% \usepackage{hyperref}
% \hypersetup{colorlinks=true, linkcolor=blue!70!black, urlcolor=blue!70!black}

% ============================================================
%  Styles Mini-HPL (listings + tcolorbox)
% ============================================================

\definecolor{codebg}{RGB}{245,245,245}
\definecolor{keywd}{RGB}{0,0,180}
\definecolor{comment}{RGB}{60,120,60}
\definecolor{string}{RGB}{160,0,0}

\lstdefinestyle{cstyle}{
  language=C,
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small,
  keywordstyle=\color{keywd}\bfseries,
  commentstyle=\color{comment}\itshape,
  stringstyle=\color{string},
  numberstyle=\tiny\color{gray},
  numbers=left, stepnumber=1, numbersep=8pt,
  breaklines=true,
  frame=single, framerule=0.4pt, rulecolor=\color{gray!50},
  tabsize=4, showstringspaces=false, captionpos=b,
  morekeywords={MPI_Init,MPI_Finalize,MPI_Comm_rank,MPI_Comm_size,
                MPI_Bcast,MPI_Barrier,MPI_Wtime,MPI_Allreduce,
                MPI_Sendrecv,MPI_Reduce,MPI_Abort,
                MPI_COMM_WORLD,MPI_DOUBLE,MPI_DOUBLE_INT,
                MPI_MAXLOC,MPI_MAX,MPI_STATUS_IGNORE}
}
\lstset{style=cstyle}

\newtcolorbox{remarque}[1][]{
  colback=yellow!10, colframe=orange!70!black,
  fonttitle=\bfseries, title=Remarque, breakable, #1}
\newtcolorbox{adef}[1][]{
  colback=blue!5, colframe=blue!50!black,
  fonttitle=\bfseries, title=Définition, breakable, #1}
\newtcolorbox{exemple}[1][]{
  colback=green!5, colframe=green!50!black,
  fonttitle=\bfseries, title=Exemple à la main, breakable, #1}
\newtcolorbox{correction}[1][]{
  colback=red!5, colframe=red!60!black,
  fonttitle=\bfseries, title=Correction appliquée, breakable, #1}

% ============================================================
\begin{document}

% ===================== PAGE DE GARDE =======================
\begin{titlepage}
\begin{tikzpicture}[remember picture,overlay]

% Logo Université Paris-Saclay (haut gauche)
\node[anchor=north west, xshift=1cm, yshift=-1cm]
at (current page.north west)
{\includegraphics[width=4.2cm]{images/universite.png}};

% Logo ENS Paris-Saclay (haut droite)
\node[anchor=north east, xshift=-1cm, yshift=-1cm]
at (current page.north east)
{\includegraphics[width=4.2cm]{images/ens.png}};

% Logo Master CHPS (bas centre)
\node[anchor=south, yshift=1.5cm]
at (current page.south)
{\includegraphics[width=6.5cm]{images/chps.png}};

\end{tikzpicture}

\vspace*{5cm}

\begin{center}

{\Huge \textbf{HPL High Performance Linpack Benchmark}\par}

\vspace{1.5cm}

{\Large Projet MPNA \\[0.2cm]
Méthodes et Programmation Numérique Avancée\par}

\vspace{2cm}

{\Large \textbf{Réalisé par :}\par}

\vspace{0.5cm}

{\Large \textit{BENMALK Achraf}\par}
\vspace{0.3cm}
{\Large \textit{BEZINE Mohamed Karim}\par}

\vspace{2cm}

{\Large \textbf{Date :} \today\par}

\end{center}

\end{titlepage}

% ====== En-tête à partir d'ici (optionnel mais pratique) ======
\pagestyle{fancy}
\fancyhf{}
\lhead{\textbf{Projet MPNA — HPL / Mini-HPL}}
\rhead{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\setlength{\headheight}{15pt}

\newpage

% ====== Tables ======
\tableofcontents
\listoffigures
\listoftables
\newpage
% ==========================================================
\section{Contexte}
HPL (High Performance Linpack) est un benchmark fondamental en calcul haute performance qui évalue la capacité de calcul en virgule flottante d’un système. Il est largement utilisé pour mesurer l’efficacité des CPUs et des GPUs lors de la résolution de grands systèmes linéaires denses.

HPL ne mesure pas uniquement la performance brute en gigaflops ou teraflops. Il permet également d’évaluer la gestion du parallélisme, les accès mémoire ainsi que le coût des communications. Il constitue ainsi un outil essentiel pour l’optimisation et la comparaison des systèmes HPC.

% ==========================================================
\section{Caractéristiques du Benchmark HPL}

HPL mesure la vitesse de résolution d’un système linéaire dense de la forme :

\[
Ax = b
\]

où \(A\) est une matrice dense de grande taille.

Il s’agit du benchmark utilisé pour classer les supercalculateurs dans la liste TOP500.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{images/hpl-parameters.png}
    \caption{Paramètres principaux du benchmark HPL}
\end{figure}

Les paramètres principaux sont :

\begin{itemize}
    \item \textbf{N} : taille globale de la matrice.
    \item \textbf{NB} : taille des blocs.
    \item \textbf{P × Q} : grille de processus MPI.
\end{itemize}

La matrice est distribuée selon une stratégie \textbf{block-cyclic} afin d’équilibrer la charge entre les processus et de minimiser les communications.

% ==========================================================
\section{Fichier d'entrée HPL.dat}

Le benchmark nécessite un fichier de configuration nommé \texttt{HPL.dat}.  
Ce fichier définit :

\begin{itemize}
    \item Les tailles de matrices testées
    \item Les tailles de blocs
    \item La grille de processus MPI
    \item Les paramètres algorithmiques
    \item La tolérance de vérification du résidu
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.9]{images/hpl-dat.png}
    \caption{Exemple de fichier HPL.dat}
\end{figure}

HPL explore différentes combinaisons de paramètres afin d’identifier la configuration optimale.

% ==========================================================
\section{Plateforme d'exécution}

Les expériences ont été réalisées sur un \textbf{cluster HPC} équipé de nœuds GPU. Les partitions utilisées sont :

\begin{itemize}
    \item \textbf{GPU A100} : partition \texttt{gpu}, NVIDIA A100 80\,Go HBM2e (architecture Ampere)
    \item \textbf{GPU H100} : partition \texttt{gpu\_h100}, NVIDIA H100 PCIe 80\,Go HBM3 (architecture Hopper)
\end{itemize}

Le benchmark est exécuté via le conteneur \textbf{NVIDIA HPC-Benchmarks 23.10}, déployé avec Singularity. Ce conteneur fournit une version optimisée de HPL qui exploite les \textbf{Tensor Cores FP64} des GPUs pour les opérations DGEMM, ainsi que les bibliothèques cuBLAS et NCCL pour les communications inter-GPU.

La configuration HPL utilisée pour les tests GPU est : $\text{NB} = 576$, $\text{P} \times \text{Q} = 2 \times 1$ (2 processus MPI, 1 par GPU), $\text{BCAST} = 6$ (broadcast MPI). La taille de bloc $\text{NB} = 576$ est nettement supérieure aux valeurs CPU typiques (128--256) car les GPUs nécessitent de grands blocs pour alimenter leurs milliers de cœurs en données et masquer la latence mémoire.

L'ordonnanceur de tâches \textbf{Slurm} est utilisé pour l'allocation des ressources GPU.

% ==========================================================
\section{Étapes d'exécution}

Les étapes principales sont :

\begin{enumerate}
    \item Création du fichier \texttt{HPL.dat}
    \item Lancement du conteneur Singularity
    \item Allocation des ressources GPU via Slurm
    \item Exécution du benchmark avec MPI
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step1.png}
    \caption{Création du fichier HPL.dat}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step2.png}
    \caption{Binding du dossier dans le conteneur}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step3.png}
    \caption{Configuration persistante du bind}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{images/Step4.png}
    \caption{Exécution du benchmark}
\end{figure}

Commande principale (pour 1 GPU, 1 processus MPI) :

\begin{verbatim}
mpirun -np 1 ./hpl.sh --dat /mnt/HPL.dat
\end{verbatim}

Pour les configurations multi-GPU, le nombre de processus MPI est ajusté (\texttt{-np 2} pour 2 GPUs).

% ==========================================================
\section{Partition A100}

\subsection{1 GPU, 1 Processus MPI}

\begin{verbatim}
salloc -t 4:00:00 -n 1 --ntasks=1 -p gpu --gres=gpu:1
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.51 & 9731 \\
40000 & 1.63 & 15890 \\
60000 & 8.36 & 17150 \\
80000 & 19.01 & 17600 \\
100000 & 36.94 & 17860 \\
\hline
\end{tabular}
\caption{Résultats sur 1 GPU A100}
\end{table}

Le pic théorique FP64 de l'A100 est calculé à partir du nombre de cœurs CUDA, du facteur FMA (\textit{Fused Multiply-Add}, chaque opération FMA compte pour 2 opérations flottantes) et de la fréquence boost :

\[
\text{Peak}_{A100} = \underbrace{6\,912}_{\substack{\text{cœurs} \\ \text{CUDA}}} \times \underbrace{2}_{\text{FMA}} \times \underbrace{1{,}41 \text{ GHz}}_{\text{freq. boost}} \approx 19{,}5 \text{ TFLOPS}
\]

Efficacité maximale atteinte (à $N = 100\,000$) :

\[
\eta_{A100} = \frac{17\,860}{19\,487} \times 100 \approx 91{,}7\%
\]

Cette efficacité élevée confirme que HPL est un benchmark \textit{compute-bound} : la majorité du temps est consacrée aux opérations DGEMM qui exploitent pleinement les capacités de calcul du GPU. Tous les tests ont passé la vérification du résidu ($\|Ax - b\|_\infty / (\varepsilon \cdot (\|A\|_\infty \cdot \|x\|_\infty + \|b\|_\infty) \cdot N) < 16{,}0$), confirmant la validité numérique des résultats.

% ==========================================================
\subsection{2 GPU, 2 Processus MPI, 1 Processus MPI/GPU}

\begin{verbatim}
salloc -t 4:00:00 -n 1 --gres=gpu:2
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.55 & 9106 \\
40000 & 1.66 & 25230 \\
60000 & 4.56 & 31430 \\
80000 & 9.98 & 33560 \\
100000 & 19.00 & 34720 \\
\hline
\end{tabular}
\caption{Résultats sur 2 GPUs A100}
\end{table}

Speedup à $N = 100\,000$ :

\[
S_{A100} = \frac{34\,720}{17\,860} \approx 1{,}95
\]

Efficacité parallèle :

\[
E_{A100} = \frac{S_{A100}}{2} \times 100 = \frac{1{,}95}{2} \times 100 \approx 97\%
\]

\textbf{Remarque :} À $N = 20\,000$, les 2 GPUs (9\,106 GFLOPS) sont \textit{plus lents} que le GPU unique (9\,731 GFLOPS). Ce phénomène est analysé en section~\ref{sec:analyse}.

% ==========================================================
\section{Partition H100}

\subsection{1 GPU, 1 Processus MPI}

\begin{verbatim}
salloc -t 4:00:00 -n 1 -p gpu_h100 --gres=gpu:1
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.31 & 16130 \\
40000 & 1.17 & 35760 \\
60000 & 3.43 & 41760 \\
80000 & 7.57 & 44130 \\
100000 & 11.43 & 45110 \\
\hline
\end{tabular}
\caption{Résultats sur 1 GPU H100}
\end{table}

Le pic théorique FP64 du H100 est calculé de manière analogue à l'A100 :

\[
\text{Peak}_{H100} = \underbrace{16\,896}_{\substack{\text{cœurs} \\ \text{CUDA}}} \times \underbrace{2}_{\text{FMA}} \times \underbrace{1{,}6 \text{ GHz}}_{\text{freq. boost}} \approx 54 \text{ TFLOPS}
\]

Efficacité maximale atteinte (à $N = 100\,000$) :

\[
\eta_{H100} = \frac{45\,110}{54\,067} \times 100 \approx 83{,}4\%
\]

L'efficacité du H100 est inférieure à celle de l'A100 (83{,}4\% vs 91{,}7\%). Le H100 possède 2{,}4$\times$ plus de cœurs CUDA que l'A100 (16\,896 vs 6\,912), ce qui rend plus difficile la saturation complète de toutes les unités de calcul pour une même taille de problème. Pour atteindre une efficacité comparable, des valeurs de $N$ encore plus grandes seraient nécessaires.

% ==========================================================
\subsection{2 GPU, 2 Processus MPI, 1 Processus MPI/GPU}

\begin{verbatim}
salloc -t 4:00:00 -n 1 --ntasks=2 -p gpu_h100 --gres=gpu:2
\end{verbatim}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
N & Temps (s) & Gflops \\
\hline
20000 & 0.40 & 11510 \\
40000 & 1.01 & 41460 \\
60000 & 1.11 & 64700 \\
80000 & 4.36 & 76730 \\
100000 & 7.95 & 81970 \\
\hline
\end{tabular}
\caption{Résultats sur 2 GPUs H100}
\end{table}

Speedup à $N = 100\,000$ :

\[
S_{H100} = \frac{81\,970}{45\,110} \approx 1{,}82
\]

Efficacité parallèle :

\[
E_{H100} = \frac{S_{H100}}{2} \times 100 = \frac{1{,}82}{2} \times 100 \approx 91\%
\]

\textbf{Remarque :} Le passage multi-GPU est moins efficace sur H100 (91\%) que sur A100 (97\%). À $N = 20\,000$, les 2 GPUs H100 (11\,510 GFLOPS) sont même 28{,}6\% \textit{plus lents} que le GPU unique (16\,130 GFLOPS). Cette dégradation est analysée en section~\ref{sec:analyse}. Tous les tests ont passé la vérification du résidu.

% ==========================================================
\section{Analyse des résultats}
\label{sec:analyse}

\subsection{Évolution des performances avec la taille du problème}

Sur les deux architectures, les performances (GFLOPS) augmentent avec $N$ avant de converger vers un plateau. Ce comportement s'explique par le rapport entre calcul et communication :

\begin{itemize}
    \item Le volume de calcul de la factorisation LU croît en $\mathcal{O}(N^3)$.
    \item Le volume de communication croît en $\mathcal{O}(N^2)$.
\end{itemize}

Ainsi, pour des grandes valeurs de $N$, le calcul domine largement et le GPU peut maintenir un taux d'utilisation élevé de ses unités de calcul (principalement via les opérations DGEMM). Pour les petites valeurs de $N$, le \textit{overhead} de communication et de synchronisation représente une fraction significative du temps total, ce qui réduit les GFLOPS mesurés.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/hpl-a100.png}
    \caption{Performances A100 : 1 GPU vs 2 GPUs}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/hpl-h100.png}
    \caption{Performances H100 : 1 GPU vs 2 GPUs}
\end{figure}

\subsection{Passage à l'échelle multi-GPU}

Le tableau~\ref{tab:scaling} résume le passage à l'échelle pour $N = 100\,000$ :

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
GPU & 1 GPU (GFLOPS) & 2 GPUs (GFLOPS) & Speedup & Efficacité parallèle \\
\hline
A100 & 17\,860 & 34\,720 & 1{,}95$\times$ & 97\% \\
H100 & 45\,110 & 81\,970 & 1{,}82$\times$ & 91\% \\
\hline
\end{tabular}
\caption{Passage à l'échelle multi-GPU à $N = 100\,000$}
\label{tab:scaling}
\end{table}

L'A100 affiche une meilleure efficacité parallèle (97\%) que le H100 (91\%). Cette différence s'explique par le fait que le H100, étant plus rapide en calcul pur, termine la phase de calcul plus rapidement, ce qui rend la latence de communication inter-GPU (via NVLink) proportionnellement plus coûteuse. Le rapport calcul/communication est donc moins favorable sur H100 pour une même taille de problème.

\subsection{Anomalie à $N = 20\,000$ : 2 GPUs plus lents qu'un seul}

Un résultat remarquable concerne la dégradation de performance à $N = 20\,000$ en configuration multi-GPU :

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
GPU & 1 GPU (GFLOPS) & 2 GPUs (GFLOPS) & Dégradation \\
\hline
A100 & 9\,731 & 9\,106 & $-6{,}4\%$ \\
H100 & 16\,130 & 11\,510 & $-28{,}6\%$ \\
\hline
\end{tabular}
\caption{Dégradation de performance à $N = 20\,000$ en multi-GPU}
\end{table}

Avec un problème de petite taille ($N = 20\,000$), chaque GPU reçoit une portion de matrice trop réduite pour saturer ses milliers de cœurs CUDA. Le coût de synchronisation et d'échange de données entre les deux GPUs dépasse alors le bénéfice du parallélisme supplémentaire. Ce phénomène est plus prononcé sur H100 ($-28{,}6\%$) car ce GPU possède davantage de cœurs CUDA à alimenter en données (16\,896 vs 6\,912).

Il existe donc un seuil de taille de problème en dessous duquel l'ajout de GPUs est contre-productif. Nos données indiquent que ce seuil se situe entre $N = 20\,000$ (dégradation observée) et $N = 40\,000$ (gain observé) pour les deux architectures.

\subsection{Comparaison architecturale A100 vs H100}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/a100-h100-hpl.png}
    \caption{Comparaison des performances A100 vs H100 (1 GPU)}
\end{figure}

Le tableau~\ref{tab:speedup_arch} présente le facteur d'accélération du H100 par rapport à l'A100 pour chaque taille de problème :

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
N & A100 (GFLOPS) & H100 (GFLOPS) & Ratio H100/A100 \\
\hline
20\,000  & 9\,731  & 16\,130 & 1{,}66$\times$ \\
40\,000  & 15\,890 & 35\,760 & 2{,}25$\times$ \\
60\,000  & 17\,150 & 41\,760 & 2{,}43$\times$ \\
80\,000  & 17\,600 & 44\,130 & 2{,}51$\times$ \\
100\,000 & 17\,860 & 45\,110 & 2{,}53$\times$ \\
\hline
\end{tabular}
\caption{Facteur d'accélération H100 par rapport à A100}
\label{tab:speedup_arch}
\end{table}

Le ratio converge vers $\approx 2{,}5\times$ pour les grandes tailles de problème. Cela est cohérent avec le rapport des pics théoriques FP64 :

\[
\frac{\text{Peak}_{H100}}{\text{Peak}_{A100}} = \frac{54\,067}{19\,487} \approx 2{,}77
\]

Le facteur mesuré (2{,}53$\times$) est proche du facteur théorique (2{,}77$\times$), avec un écart de $\approx 9\%$. Cet écart s'explique par la différence d'efficacité entre les deux architectures : l'A100 atteint 91{,}7\% de son pic tandis que le H100 n'atteint que 83{,}4\%, car il est plus difficile de saturer un GPU disposant de davantage de cœurs.

À $N = 20\,000$, le ratio n'est que de 1{,}66$\times$ car les deux GPUs sont sous-utilisés : ni l'un ni l'autre ne peut exploiter pleinement ses unités de calcul avec un problème aussi petit.

\subsection{Synthèse de l'efficacité}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Configuration & Pic théorique TC (TFLOPS) & Meilleur résultat (GFLOPS) & Efficacité \\
\hline
A100 (1 GPU) & 19{,}5 & 17\,860 & 91{,}7\% \\
H100 (1 GPU) & 54{,}0 & 45\,110 & 83{,}4\% \\
A100 (2 GPUs) & 39{,}0 & 34\,720 & 89{,}0\% \\
H100 (2 GPUs) & 108{,}0 & 81\,970 & 75{,}8\% \\
\hline
\end{tabular}
\caption{Synthèse de l'efficacité HPL sur les différentes configurations}
\label{tab:efficacite}
\end{table}

On observe que l'efficacité décroît lorsqu'on passe à des configurations plus puissantes. Plus le pic théorique est élevé, plus il est difficile de l'exploiter pleinement. L'A100 atteint une excellente efficacité de 91{,}7\% grâce à un ratio cœurs/performance mieux maîtrisé, tandis que le H100 2 GPUs descend à 75{,}8\%. Cela témoigne néanmoins de la nature \textit{compute-bound} du benchmark HPL et de la qualité de l'implémentation GPU fournie par NVIDIA dans le conteneur HPC-Benchmarks.

% ==========================================================
\subsection{Conclusion}

Ce travail a permis d'évaluer les performances du benchmark HPL sur deux architectures GPU NVIDIA --- l'A100 (Ampere) et le H100 (Hopper) --- au sein d'un cluster HPC.

Les principaux enseignements sont :

\begin{enumerate}
    \item \textbf{HPL est un benchmark compute-bound :} Les efficacités mesurées (83--92\%) confirment que la performance est principalement limitée par la puissance de calcul et non par la bande passante mémoire ou les communications.

    \item \textbf{La taille du problème est déterminante :} Les performances augmentent significativement avec $N$ en raison du rapport $\mathcal{O}(N^3)/\mathcal{O}(N^2)$ entre calcul et communication. Un $N$ trop petit conduit à une sous-utilisation des ressources.

    \item \textbf{Le multi-GPU a un seuil de rentabilité :} En dessous d'un certain $N$ (entre 20\,000 et 40\,000 dans nos tests), l'ajout d'un second GPU dégrade les performances au lieu de les améliorer.

    \item \textbf{Le H100 offre un gain de $\approx 2{,}5\times$ sur l'A100 :} Ce ratio mesuré (2{,}53$\times$) est cohérent avec le rapport des pics théoriques FP64 ($2{,}77\times$), l'écart de $\approx 9\%$ s'expliquant par la différence d'efficacité entre les deux architectures.

    \item \textbf{L'efficacité parallèle diminue avec la puissance :} L'A100 affiche une meilleure efficacité parallèle multi-GPU (97\%) que le H100 (91\%), car la latence de communication devient proportionnellement plus coûteuse sur un GPU plus rapide.
\end{enumerate}

Ces résultats illustrent les compromis fondamentaux du calcul haute performance : puissance brute, passage à l'échelle, et dimensionnement du problème sont étroitement liés. L'optimisation des paramètres HPL (taille de matrice, taille de bloc, grille de processus) reste essentielle pour exploiter au mieux les architectures modernes.
% ==========================================================
%  SECTION INTÉGRÉE : Mini-HPL (version longue pédagogique)
% ==========================================================
\section{Mini-HPL parallèle avec MPI}
\label{sec:mini-hpl}
\subsection{Rappels mathématiques}

\subsubsection*{Système linéaire et élimination de Gauss}

On cherche $x \in \mathbb{R}^N$ tel que $Ax=b$,
avec $A \in \mathbb{R}^{N\times N}$ inversible.
On travaille sur la matrice augmentée $[A \mid b]$.

À l'étape $k$, on annule les coefficients de la colonne $k$ pour toutes les lignes $i>k$ :
\begin{equation}
  \ell_{ik} = \frac{a_{ik}}{a_{kk}}, \qquad
  \text{ligne}_i \leftarrow \text{ligne}_i - \ell_{ik}\cdot\text{ligne}_k
\end{equation}

\subsubsection*{Complexité et GFLOPS}

Le nombre d'opérations flottantes est $W \approx \frac{2}{3}N^3$.
La performance est donc :
\[
  \text{GFLOPS}_{\text{approx}} =
  \frac{\frac{2}{3}N^3}{t_{\text{élim}} \times 10^9}.
\]

\subsubsection*{Pivotage partiel}

Sans pivotage, l'algorithme est instable si $a_{kk}$ est petit.
Le pivotage partiel permute la ligne $k$ avec la ligne $p$ telle que :
\[
  p = \arg\max_{i \geq k} |a_{ik}|.
\]

\subsection{Exemple à la main — Gauss avec pivotage ($N=3$)}

\begin{exemple}
Soit le système $Ax=b$ avec :
\[
A = \begin{pmatrix} 1&2&1\\3&8&1\\0&4&1 \end{pmatrix},\quad
b = \begin{pmatrix}2\\12\\2\end{pmatrix}.
\]

\textbf{Étape $k=0$} : max en colonne 0 = $|a_{10}|=3$.
On échange $L_0 \leftrightarrow L_1$ :
\[
\left(\begin{array}{ccc|c}3&8&1&12\\1&2&1&2\\0&4&1&2\end{array}\right).
\]
$\ell_{10}=\frac{1}{3}$, $\ell_{20}=0$.
$L_1 \leftarrow L_1 - \frac{1}{3}L_0$ :
\[
\left(\begin{array}{ccc|c}3&8&1&12\\0&-\frac{2}{3}&\frac{2}{3}&-2\\0&4&1&2\end{array}\right).
\]

\textbf{Étape $k=1$} : max en colonne 1 (lignes $\geq 1$) = $|a_{21}|=4$.
Échange $L_1 \leftrightarrow L_2$ :
\[
\left(\begin{array}{ccc|c}3&8&1&12\\0&4&1&2\\0&-\frac{2}{3}&\frac{2}{3}&-2\end{array}\right).
\]
$\ell_{21}=\frac{-2/3}{4}=-\frac{1}{6}$.
$L_2 \leftarrow L_2 + \frac{1}{6}L_1$ :
\[
\left(\begin{array}{ccc|c}3&8&1&12\\0&4&1&2\\0&0&\frac{5}{6}&-\frac{5}{3}\end{array}\right).
\]

\textbf{Back-substitution} :
$x_2 = \frac{-5/3}{5/6} = -2$,\quad
$x_1 = \frac{2 - 1\cdot(-2)}{4} = 1$,\quad
$x_0 = \frac{12 - 8\cdot 1 - 1\cdot(-2)}{3} = 2$.

\textbf{Solution} : $x=(2,1,-2)^\top$. \checkmark
\end{exemple}

\subsection{Architecture MPI : distribution bloc-cyclique}

\begin{adef}
Avec $P$ processus et une taille de bloc $NB$, la ligne globale $i$
appartient au processus :
\[
  \texttt{OWNER}(i) = \left\lfloor \frac{i}{NB} \right\rfloor \bmod P.
\]
Son indice local sur ce processus est :
\[
  \texttt{LOCAL\_LI}(i) =
  \left\lfloor \frac{\lfloor i/NB \rfloor}{P} \right\rfloor \cdot NB
  + (i \bmod NB).
\]
\end{adef}

\begin{exemple}
$N=12$, $NB=2$, $P=3$.
\begin{center}
\begin{tabular}{c|cccccccccccc}
\toprule
Ligne globale & 0&1&2&3&4&5&6&7&8&9&10&11\\
\midrule
Processus     & 0&0&1&1&2&2&0&0&1&1&2&2\\
\bottomrule
\end{tabular}
\end{center}
\end{exemple}

\begin{remarque}[title={Contrainte simplificatrice}]
Dans notre Mini-HPL, $N$ doit être divisible par $NB \times P$.
Cela évite la gestion des blocs partiels (présente dans ScaLAPACK/HPL).
\end{remarque}

\subsection{Explication du code}

\subsubsection*{Macros d'indexation}

\begin{lstlisting}[caption={Macros définies avant main()}]
/* Acces element (i,j) dans tableau row-major de largeur N */
#define IDX(i, j, N)  ((i) * (N) + (j))

/* Distribution bloc-cyclique (NB et size visibles dans la portee) */
#define OWNER(gi)     (((gi) / NB) % size)
#define LOCAL_LI(gi)  ((((gi) / NB) / size) * NB + (gi) % NB)
\end{lstlisting}

\subsubsection*{Initialisation}

\begin{lstlisting}[caption={Matrice diagonalement dominante}]
void initialize_matrix(double *A, double *b,
                        int N, int NB,
                        int local_rows, int rank, int size)
{
    for (int li = 0; li < local_rows; li++) {
        int panel_local  = li / NB;
        int within_panel = li % NB;
        int global_i = (panel_local * size + rank) * NB + within_panel;
        for (int j = 0; j < N; j++) {
            if (j == global_i)
                A[IDX(li,j,N)] = (double)N + (double)(global_i+j+1)/N;
            else
                A[IDX(li,j,N)] = (double)(global_i+j+1)/N;
        }
        b[li] = 1.0;
    }
}
\end{lstlisting}

\subsubsection*{Étape 1 — Recherche du pivot : \texttt{MPI\_DOUBLE\_INT}}

\begin{lstlisting}[caption={Réduction MAXLOC (type standard C)}]
struct { double val; int idx; } local_cand, global_cand;
local_cand.val = 0.0;
local_cand.idx = -1;

/* ... calcul local du max dans la colonne k ... */

MPI_Allreduce(&local_cand, &global_cand, 1,
              MPI_DOUBLE_INT, MPI_MAXLOC, MPI_COMM_WORLD);

int pivot_gi    = global_cand.idx;
int pivot_owner = OWNER(pivot_gi);
int pivot_li    = LOCAL_LI(pivot_gi);
\end{lstlisting}


\subsubsection*{Étape 2 — Échange de lignes (\texttt{MPI\_Sendrecv})}

\begin{lstlisting}[caption={Swap entre processus sans deadlock}]
MPI_Sendrecv(tmp_row,  N+1, MPI_DOUBLE, pivot_owner, 0,
             pivot_row, N+1, MPI_DOUBLE, pivot_owner, 0,
             MPI_COMM_WORLD, MPI_STATUS_IGNORE);
\end{lstlisting}

\subsubsection*{Étape 3 — Broadcast de la ligne pivot}

\begin{lstlisting}[caption={Diffusion de la ligne pivot + b}]
MPI_Bcast(pivot_row, N+1, MPI_DOUBLE, k_owner, MPI_COMM_WORLD);
\end{lstlisting}

\subsubsection*{Vérification du pivot et arrêt propre}

\begin{lstlisting}[caption={Arrêt immédiat si pivot quasi-nul}]
double pivot_val = pivot_row[k];
if (fabs(pivot_val) < 1e-14) {
    if (rank == 0)
        fprintf(stderr,
            "ERREUR : pivot quasi-nul a k=%d (|pivot|=%.2e). Arret.\n",
            k, fabs(pivot_val));
    MPI_Abort(MPI_COMM_WORLD, 2);
}
\end{lstlisting}

\subsubsection*{Étape 4 — Mise à jour SAXPY}

\begin{lstlisting}[caption={Mise a jour in-place + LU}]
double factor = A[IDX(li, k, N)] / pivot_val;
for (int j = k; j < N; j++)
    A[IDX(li, j, N)] -= factor * pivot_row[j];
b_vec[li]        -= factor * pivot_row[N];
A[IDX(li, k, N)]  = factor;   /* stocke L in-place */
\end{lstlisting}

\subsection{Back-substitution}

On résout $Ux=b$ de $k=N-1$ à $0$ :
\[
  x_k = \frac{b_k - \sum_{j=k+1}^{N-1} u_{kj}x_j}{u_{kk}}.
\]

\begin{lstlisting}[caption={Back-substitution parallèle}]
MPI_Bcast(&x_k, 1, MPI_DOUBLE, k_owner, MPI_COMM_WORLD);
\end{lstlisting}

\subsection{Résiduelle normée HPL}

\[
  r = \frac{\|Ax - b\|_\infty}{\|A\|_\infty \cdot \|x\|_\infty
      \cdot N \cdot \varepsilon}
  \qquad \text{PASSED si } r < 16,
\]
avec $\varepsilon = 2{,}22 \times 10^{-16}$.

\begin{lstlisting}[caption={Réduction des normes (MPI_MAX)}]
MPI_Reduce(&local_res_norm, &global_res_norm, 1,
           MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);
MPI_Reduce(&local_A_norm, &global_A_norm, 1,
           MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);
\end{lstlisting}

\subsection{Tableau récapitulatif des communications MPI}

\begin{center}
\begin{tabular}{llll}
  \toprule
  \textbf{Fonction} & \textbf{Type} & \textbf{Étape} & \textbf{Rôle} \\
  \midrule
  \texttt{MPI\_Allreduce} & \texttt{MPI\_DOUBLE\_INT} & Pivot & Pivot max global (MAXLOC) \\
  \texttt{MPI\_Sendrecv}  & \texttt{MPI\_DOUBLE}      & Swap  & Échange de lignes \\
  \texttt{MPI\_Bcast}     & \texttt{MPI\_DOUBLE}      & Gauss & Diffusion ligne pivot \\
  \texttt{MPI\_Bcast}     & \texttt{MPI\_DOUBLE}      & Back-sub & Diffusion $x_k$ \\
  \texttt{MPI\_Reduce}    & \texttt{MPI\_MAX}         & Check & Normes globales \\
  \texttt{MPI\_Barrier}   & --                        & Timing & Synchronisation \\
  \bottomrule
\end{tabular}
\end{center}

\subsection{Compilation et exécution}

\begin{lstlisting}[language=bash, caption={Compilation et lancement}]
mpicc -O2 -Wall -o mini_hpl mini_hpl.c -lm
mpirun -np 4 ./mini_hpl 1024
# Contrainte : N divisible par NB*P
\end{lstlisting}

\subsection{Conclusion}

Cette implémentation du Mini-HPL parallèle illustre les principes fondamentaux du benchmark HPL : élimination de Gauss avec pivotage partiel, distribution bloc-cyclique des données et coordination via communications MPI. Les opérations collectives (\texttt{MPI\_Allreduce}, \texttt{MPI\_Bcast}, \texttt{MPI\_Reduce}) assurent la cohérence globale du calcul, tandis que le coût dominant reste en $\mathcal{O}(N^3)$. La vérification par la résiduelle normée garantit la validité numérique des résultats. Ce Mini-HPL constitue ainsi une base pédagogique pour comprendre le fonctionnement interne d’un benchmark HPC parallèle.

% ==========================================================
\begin{thebibliography}{9}

\bibitem{hpl}
A. Petitet, R. C. Whaley, J. Dongarra, A. Cleary, \textit{HPL --- A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers}, version 2.3, 2018. \url{https://www.netlib.org/benchmark/hpl/}

\bibitem{top500}
TOP500 Project, \textit{TOP500 Supercomputer Sites}. \url{https://www.top500.org/}

\bibitem{nvidia_a100}
NVIDIA Corporation, \textit{NVIDIA A100 Tensor Core GPU --- Datasheet}, 2020. \url{https://www.nvidia.com/en-us/data-center/a100/}

\bibitem{nvidia_h100}
NVIDIA Corporation, \textit{NVIDIA H100 Tensor Core GPU --- Datasheet}, 2022. \url{https://www.nvidia.com/en-us/data-center/h100/}

\bibitem{hpc_benchmarks}
NVIDIA Corporation, \textit{NVIDIA HPC-Benchmarks Container}, NGC Catalog. \url{https://catalog.ngc.nvidia.com/orgs/nvidia/containers/hpc-benchmarks}

\end{thebibliography}

\end{document}
